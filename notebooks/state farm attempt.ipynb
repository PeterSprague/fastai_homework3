{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Statefarm Kaggle submission (fast.ai homework3)\n",
    "What I'll need to do:\n",
    "* set up data structure into sample, train, valid, test\n",
    "* Import VGG16\n",
    "* pop the top layer, train it\n",
    "* set all fully connected layers to trainable\n",
    "* Improvements:\n",
    "    * play with dropout parameter\n",
    "    * add data augmentation\n",
    "    * stack multiple versions of the classifier\n",
    "    * apply batch norm\n",
    "    * have a setup that adjusts learning rate\n",
    "    \n",
    "These are general imports, always make sure to run these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import csv\n",
    "os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model_url = \"http://files.fast.ai/models/\"\n",
    "model_name = \"vgg16.h5\"\n",
    "cache_dir = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data structure\n",
    "First we set up the data structure, with proper:\n",
    "* sample \n",
    "* train\n",
    "* validation\n",
    "* test \n",
    "directories in the `processed` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_path = os.path.join(os.getcwd(), os.pardir, 'data', 'raw')\n",
    "processed_path = os.path.join(os.getcwd(), os.pardir, 'data', 'processed')\n",
    "\n",
    "# Make directories sample, valid, train, test, first check if this whole step is necessary\n",
    "if os.path.exists(os.path.join(processed_path, 'sample')):\n",
    "    print 'Sample directory already exists, no need to do data structuring!'\n",
    "else:\n",
    "    os.mkdir(os.path.join(processed_path, 'sample'))\n",
    "    os.mkdir(os.path.join(processed_path, 'sample', 'train'))\n",
    "    os.mkdir(os.path.join(processed_path, 'sample', 'valid'))\n",
    "    os.mkdir(os.path.join(processed_path, 'valid'))\n",
    "    \n",
    "    # Extract Kaggle zipfiles to correct path\n",
    "    print 'Extracting zips, this may take a while...'\n",
    "    img_zip_handle = zipfile.ZipFile(os.path.join(raw_path, 'imgs.zip'), 'r')\n",
    "    img_zip_handle.extractall(processed_path)\n",
    "    img_zip_handle.close()\n",
    "    \n",
    "    csv_zip_handle = zipfile.ZipFile(os.path.join(raw_path, 'driver_imgs_list.csv.zip'), 'r')\n",
    "    csv_zip_handle.extractall(processed_path)\n",
    "    csv_zip_handle.close()\n",
    "    print 'Done extracting zips!'\n",
    "    \n",
    "    # Set up sample directory structure\n",
    "    for i in range(10):\n",
    "        dirname = 'c' + str(i)\n",
    "        os.mkdir(os.path.join(processed_path, 'sample', 'train', dirname))\n",
    "        os.mkdir(os.path.join(processed_path, 'sample', 'valid', dirname))\n",
    "        os.mkdir(os.path.join(processed_path, 'valid', dirname))\n",
    "        \n",
    "    data = np.genfromtxt(os.path.join(processed_path, 'driver_imgs_list.csv'), delimiter=',', dtype=None)\n",
    "    data = data[1:,:]\n",
    "    drivers = np.unique(data[:,0])\n",
    "    sample_drivers = np.random.choice(drivers, 3, replace=False)\n",
    "    for i in range(2):\n",
    "        driver_name = sample_drivers[i]\n",
    "        driver_columns = data[data[:,0] == driver_name]\n",
    "        for j in range(10):\n",
    "            driver_class = 'c' + str(j)\n",
    "            dest = os.path.join(processed_path, 'sample', 'train', driver_class)\n",
    "\n",
    "            class_columns = driver_columns[driver_columns[:,1] == driver_class]\n",
    "            filenames = np.random.choice(class_columns[:,2], 3, replace=False)\n",
    "            for filename in filenames:\n",
    "                src = os.path.join(processed_path, 'train', driver_class, filename)\n",
    "                shutil.copyfile(src, os.path.join(dest, filename))\n",
    "\n",
    "    # Fill in validation driver\n",
    "    driver_name = sample_drivers[2]\n",
    "    driver_columns = data[data[:,0] == driver_name]\n",
    "    for j in range(10):\n",
    "        driver_class = 'c' + str(j)\n",
    "        dest = os.path.join(processed_path, 'sample', 'valid', driver_class)\n",
    "\n",
    "        class_columns = driver_columns[driver_columns[:,1] == driver_class]\n",
    "        filenames = np.random.choice(class_columns[:,2], 3, replace=False)\n",
    "        for filename in filenames:\n",
    "            src = os.path.join(processed_path, 'train', driver_class, filename)\n",
    "            shutil.copyfile(src, os.path.join(dest, filename))\n",
    "            \n",
    "    # Throw 30% of train data into valid folder\n",
    "    num_drivers = drivers.shape[0]\n",
    "    validation_drivers_amount = int(np.floor(num_drivers*0.3))\n",
    "    validation_drivers = np.random.choice(drivers, validation_drivers_amount, replace=False)\n",
    "\n",
    "    for i in range(validation_drivers_amount):\n",
    "        driver_name = validation_drivers[i]\n",
    "        driver_columns = data[data[:,0] == driver_name]\n",
    "\n",
    "        for j in range(10):\n",
    "            driver_class = 'c' + str(j)\n",
    "            class_columns = driver_columns[driver_columns[:,1] == driver_class]\n",
    "            for filename in class_columns[:,2]:\n",
    "                src = os.path.join(processed_path, 'train', driver_class, filename)\n",
    "                dest = os.path.join(processed_path, 'valid', driver_class, filename)\n",
    "                shutil.move(src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16() setup boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape([3,1,1])\n",
    "def vgg_preprocess(x):\n",
    "    x = x - vgg_mean\n",
    "    return x[:,:,::-1]\n",
    "\n",
    "def add_conv_block(model, layers, filters):\n",
    "    for i in range(layers):\n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    return model\n",
    "    \n",
    "def add_fc_block(model, dropout):\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class vgg16():\n",
    "    def __init__(self, dropout=0.5):\n",
    "        self.create(dropout)\n",
    "        \n",
    "    def create(self, dropout):\n",
    "        model = self.model = Sequential()\n",
    "        \n",
    "        model.add(Lambda(vgg_preprocess, input_shape=(3, 244, 244), output_shape=(3, 244, 244)))\n",
    "        \n",
    "        model = add_conv_block(model, 2, 64)\n",
    "        model = add_conv_block(model, 2, 128)\n",
    "        model = add_conv_block(model, 3, 256)\n",
    "        model = add_conv_block(model, 3, 512)\n",
    "        model = add_conv_block(model, 3, 512)\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model = add_fc_block(model, dropout)\n",
    "        model = add_fc_block(model, dropout)\n",
    "        model.add(Dense(1000, activation='softmax'))\n",
    "        \n",
    "        model = model.load_weights(get_file(model_name, model_url+model_name, cache_subdir=cache_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load in data with generators\n",
    "Here I set up the generators for the training and validation work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 images belonging to 10 classes.\n",
      "Found 30 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "DEBUG = True\n",
    "data_dir = os.path.join(os.getcwd(), os.pardir, 'data')\n",
    "model_dir = os.path.join(os.getcwd(), os.pardir, 'models')\n",
    "if DEBUG == True:\n",
    "    path = os.path.join(data_dir, 'processed', 'sample')\n",
    "    batch_size = 4\n",
    "    epochs = 2\n",
    "elif DEBUG == False:\n",
    "    path = os.path.join(data_dir, 'processed')\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "\n",
    "train_path = os.path.join(path, 'train')\n",
    "val_path = os.path.join(path, 'valid')\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(244,244), batch_size=batch_size, shuffle=True)\n",
    "val_batches = ImageDataGenerator().flow_from_directory(val_path, target_size=(244,244), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning the model\n",
    "* Now the top layer must be popped and replaced with a 10-output, which will correspond to our hot-encoding.\n",
    "* Then retrain model with new dense layer, which will be a good starting point for later fine tuning\n",
    "* Save the model, so that we can start toying with it in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60/60 [==============================] - 59s - loss: 5.3957 - acc: 0.0833 - val_loss: 3.1434 - val_acc: 0.2000\n",
      "Epoch 2/2\n",
      "60/60 [==============================] - 58s - loss: 2.5633 - acc: 0.3333 - val_loss: 3.0977 - val_acc: 0.2333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11fa41dd0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "model = vgg16(dropout=0.5).model\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=Adam(lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(train_batches, \n",
    "                    samples_per_epoch=train_batches.nb_sample, \n",
    "                    nb_epoch=epochs, \n",
    "                    validation_data=val_batches, \n",
    "                    nb_val_samples=val_batches.nb_sample)\n",
    "# model.model.save(os.path.join(model_dir, 'model_with_new_top.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New model architecture\n",
    "Now that we have the trained model, we should probably make all the FC layers trainable. Additionally, we can start playing with:\n",
    "* learning rate schedule\n",
    "* batchnorm\n",
    "* data augmentation\n",
    "* setting different epochs\n",
    "* some other kind of regularisation?\n",
    "\n",
    "First, import the model from when we saved it. Then:\n",
    "* Separate convolutional layers from fully connected ones\n",
    "* Make a new convolutional architecture with whatever we want to implement\n",
    "* Put them together\n",
    "* Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape([3,1,1])\n",
    "\n",
    "old_model = load_model(os.path.join(os.getcwd(), \n",
    "                                    os.pardir, \n",
    "                                    'models', \n",
    "                                    'model_with_new_top.h5'), \n",
    "                       custom_objects={'vgg_mean': vgg_mean})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Batch normalisation\n",
    "Let's implement batch normalisation first. It'll speed up our looking for the adequate learning rate. From [this link](https://github.com/fchollet/keras/issues/1802) we know that `BatchNorm()` needs to be applied after the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_index = [index for index,layer in enumerate(old_model.layers) if type(layer).__name__ == 'Flatten'][0]\n",
    "\n",
    "conv_model = old_model.layers[1:flatten_index-1]\n",
    "\n",
    "def add_fc_layers(conv_layers, dropout):\n",
    "    model = Sequential()\n",
    "    for layer in conv_layers:\n",
    "        model.add(layer)\n",
    "    \n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "dropout = 0.4\n",
    "model = add_fc_layers(conv_model, dropout)\n",
    "lr = 0.001\n",
    "model.compile(optimizer=Adam(lr), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "Let's set up new batch generators, this time making use of augmented data. Remember, we only seek to augment our **training** input, no need to augment validation input (there's no learning taking place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 images belonging to 10 classes.\n",
      "Found 30 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "DEBUG = True\n",
    "data_dir = os.path.join(os.getcwd(), os.pardir, 'data')\n",
    "model_dir = os.path.join(os.getcwd(), os.pardir, 'models')\n",
    "if DEBUG == True:\n",
    "    path = os.path.join(data_dir, 'processed', 'sample')\n",
    "    batch_size = 4\n",
    "    epochs = 2\n",
    "elif DEBUG == False:\n",
    "    path = os.path.join(data_dir, 'processed')\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "\n",
    "train_path = os.path.join(path, 'train')\n",
    "val_path = os.path.join(path, 'valid')\n",
    "\n",
    "train_image_gen = ImageDataGenerator(rotation_range=5,\n",
    "                                    shear_range = 1,\n",
    "                                    zoom_range=1.2,\n",
    "                                    channel_shift_range=0.3)\n",
    "train_batches = train_image_gen.flow_from_directory(train_path, \n",
    "                                                    target_size=(244,244), \n",
    "                                                    batch_size=batch_size, \n",
    "                                                    shuffle=True)\n",
    "\n",
    "val_batches = ImageDataGenerator().flow_from_directory(val_path, \n",
    "                                                       target_size=(244,244), \n",
    "                                                       batch_size=batch_size, \n",
    "                                                       shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60/60 [==============================] - 56s - loss: 13.6916 - acc: 0.0833 - val_loss: 15.5808 - val_acc: 0.0333\n",
      "Epoch 2/3\n",
      "60/60 [==============================] - 58s - loss: 14.7303 - acc: 0.0667 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/3\n",
      "60/60 [==============================] - 57s - loss: 14.7784 - acc: 0.0833 - val_loss: 15.0436 - val_acc: 0.0667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x113c99690>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 3\n",
    "lr = 0.001\n",
    "model.optimizer.lr.set_value(lr)\n",
    "model.fit_generator(train_batches, \n",
    "                    samples_per_epoch=train_batches.nb_sample, \n",
    "                    nb_epoch=epochs, \n",
    "                    validation_data=val_batches, \n",
    "                    nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 3, 244, 244)   0           lambda_input_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_1 (ZeroPadding2D)  (None, 3, 246, 246)   0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 244, 244)  1792        zeropadding2d_1[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_2 (ZeroPadding2D)  (None, 64, 246, 246)  0           convolution2d_1[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 244, 244)  36928       zeropadding2d_2[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 122, 122)  0           convolution2d_2[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_3 (ZeroPadding2D)  (None, 64, 124, 124)  0           maxpooling2d_1[3][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 122, 122) 73856       zeropadding2d_3[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_4 (ZeroPadding2D)  (None, 128, 124, 124) 0           convolution2d_3[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 122, 122) 147584      zeropadding2d_4[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 61, 61)   0           convolution2d_4[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_5 (ZeroPadding2D)  (None, 128, 63, 63)   0           maxpooling2d_2[3][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 256, 61, 61)   295168      zeropadding2d_5[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_6 (ZeroPadding2D)  (None, 256, 63, 63)   0           convolution2d_5[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 256, 61, 61)   590080      zeropadding2d_6[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_7 (ZeroPadding2D)  (None, 256, 63, 63)   0           convolution2d_6[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 256, 61, 61)   590080      zeropadding2d_7[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 30, 30)   0           convolution2d_7[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 256, 32, 32)   0           maxpooling2d_3[3][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 512, 30, 30)   1180160     zeropadding2d_8[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_9 (ZeroPadding2D)  (None, 512, 32, 32)   0           convolution2d_8[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 512, 30, 30)   2359808     zeropadding2d_9[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_10 (ZeroPadding2D) (None, 512, 32, 32)   0           convolution2d_9[3][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 512, 30, 30)   2359808     zeropadding2d_10[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 512, 15, 15)   0           convolution2d_10[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_11 (ZeroPadding2D) (None, 512, 17, 17)   0           maxpooling2d_4[3][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 512, 15, 15)   2359808     zeropadding2d_11[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_12 (ZeroPadding2D) (None, 512, 17, 17)   0           convolution2d_11[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 512, 15, 15)   2359808     zeropadding2d_12[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_13 (ZeroPadding2D) (None, 512, 17, 17)   0           convolution2d_12[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 512, 15, 15)   2359808     zeropadding2d_13[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_14 (MaxPooling2D)   (None, 512, 7, 7)     0           convolution2d_13[3][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 25088)         0           maxpooling2d_14[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_31 (Dense)                 (None, 128)           3211392     flatten_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)             (None, 128)           0           dense_31[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_32 (Dense)                 (None, 128)           16512       dropout_19[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)             (None, 128)           0           dense_32[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_33 (Dense)                 (None, 10)            1290        dropout_20[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 17,943,882\n",
      "Trainable params: 3,229,194\n",
      "Non-trainable params: 14,714,688\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
