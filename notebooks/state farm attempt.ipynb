{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statefarm Kaggle submission (fast.ai homework3)\n",
    "What I'll need to do:\n",
    "* set up data structure into sample, train, valid, test\n",
    "* Import VGG16\n",
    "* pop the top layer, train it\n",
    "* set all fully connected layers to trainable\n",
    "* Improvements:\n",
    "    * play with dropout parameter\n",
    "    * add data augmentation\n",
    "    * stack multiple versions of the classifier\n",
    "    * apply batch norm\n",
    "    * have a setup that adjusts learning rate\n",
    "    \n",
    "These are general imports, always make sure to run these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import csv\n",
    "import bcolz\n",
    "os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "model_url = \"http://files.fast.ai/models/\"\n",
    "model_name = \"vgg16.h5\"\n",
    "cache_dir = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data structure\n",
    "First we set up the data structure, with proper:\n",
    "* sample (about 10% of the data) \n",
    "* train\n",
    "* validation (about 15% of the data)\n",
    "* test \n",
    "directories in the `processed` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_path = os.path.join(os.getcwd(), os.pardir, 'data', 'raw')\n",
    "processed_path = os.path.join(os.getcwd(), os.pardir, 'data', 'processed')\n",
    "\n",
    "# Make directories sample, valid, train, test, first check if this whole step is necessary\n",
    "if os.path.exists(os.path.join(processed_path, 'sample')):\n",
    "    print 'Sample directory already exists, no need to do data structuring!'\n",
    "else:\n",
    "    os.mkdir(os.path.join(processed_path, 'sample'))\n",
    "    os.mkdir(os.path.join(processed_path, 'sample', 'train'))\n",
    "    os.mkdir(os.path.join(processed_path, 'sample', 'valid'))\n",
    "    os.mkdir(os.path.join(processed_path, 'valid'))\n",
    "    \n",
    "    # Extract Kaggle zipfiles to correct path\n",
    "    print 'Extracting zips, this may take a while...'\n",
    "    img_zip_handle = zipfile.ZipFile(os.path.join(raw_path, 'imgs.zip'), 'r')\n",
    "    img_zip_handle.extractall(processed_path)\n",
    "    img_zip_handle.close()\n",
    "    \n",
    "    csv_zip_handle = zipfile.ZipFile(os.path.join(raw_path, 'driver_imgs_list.csv.zip'), 'r')\n",
    "    csv_zip_handle.extractall(processed_path)\n",
    "    csv_zip_handle.close()\n",
    "    print 'Done extracting zips!'\n",
    "    \n",
    "    # Set up sample directory structure\n",
    "    for i in range(10):\n",
    "        dirname = 'c' + str(i)\n",
    "        os.mkdir(os.path.join(processed_path, 'sample', 'train', dirname))\n",
    "        os.mkdir(os.path.join(processed_path, 'sample', 'valid', dirname))\n",
    "        os.mkdir(os.path.join(processed_path, 'valid', dirname))\n",
    "        \n",
    "    data = np.genfromtxt(os.path.join(processed_path, 'driver_imgs_list.csv'), delimiter=',', dtype=None)\n",
    "    data = data[1:,:]\n",
    "    drivers = np.unique(data[:,0])\n",
    "    num_drivers = drivers.shape[0]\n",
    "    # Throw 15% of train data into sample folder\n",
    "    sample_drivers_amount = int(np.floor(num_drivers*0.15))\n",
    "    sample_drivers = np.random.choice(drivers, sample_drivers_amount, replace=False)\n",
    "\n",
    "    # Throw 20% of train data into valid folder\n",
    "    validation_drivers_amount = int(np.floor(num_drivers*0.2))\n",
    "    validation_drivers = np.random.choice(drivers, validation_drivers_amount, replace=False)\n",
    "\n",
    "    # Set up sample set\n",
    "    for i in range(sample_drivers_amount):\n",
    "        driver_name = sample_drivers[i]\n",
    "        driver_columns = data[data[:,0] == driver_name]\n",
    "        for j in range(10):\n",
    "            driver_class = 'c' + str(j)\n",
    "            dest = os.path.join(processed_path, 'sample', 'train', driver_class)\n",
    "            class_columns = driver_columns[driver_columns[:,1] == driver_class]\n",
    "            for filename in class_columns[:,2]:\n",
    "                src = os.path.join(processed_path, 'train', driver_class, filename)\n",
    "                shutil.copyfile(src, os.path.join(dest, filename))\n",
    "\n",
    "    # Now move from sample_train to sample_validation a fraction of ~40%\n",
    "    sample_drivers_validation_amount = int(np.floor(sample_drivers_amount*0.4))\n",
    "    sample_drivers_validation = np.random.choice(sample_drivers, \n",
    "                                                 sample_drivers_validation_amount, \n",
    "                                                 replace=False)\n",
    "\n",
    "    for i in range(sample_drivers_validation_amount):\n",
    "        driver_name = sample_drivers_validation[i]\n",
    "        driver_columns = data[data[:,0] == driver_name]\n",
    "        for j in range(10):\n",
    "            driver_class = 'c' + str(j)\n",
    "            class_columns = driver_columns[driver_columns[:,1] == driver_class]\n",
    "            for filename in class_columns[:,2]:\n",
    "                dest = os.path.join(processed_path, 'sample', 'valid', driver_class, filename)\n",
    "                src = os.path.join(processed_path, 'sample', 'train', driver_class, filename)\n",
    "                shutil.move(src, dest)\n",
    "\n",
    "    # Set up validation set\n",
    "    for i in range(validation_drivers_amount):\n",
    "        driver_name = validation_drivers[i]\n",
    "        driver_columns = data[data[:,0] == driver_name]\n",
    "\n",
    "        for j in range(10):\n",
    "            driver_class = 'c' + str(j)\n",
    "            class_columns = driver_columns[driver_columns[:,1] == driver_class]\n",
    "            for filename in class_columns[:,2]:\n",
    "                src = os.path.join(processed_path, 'train', driver_class, filename)\n",
    "                dest = os.path.join(processed_path, 'valid', driver_class, filename)\n",
    "                shutil.move(src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# VGG16() setup boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_conv_block(model, layers, filters):\n",
    "    for i in range(layers):\n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "    return model\n",
    "    \n",
    "def add_fc_block(model, dropout):\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class vgg16():\n",
    "    def __init__(self, dropout=0.5):\n",
    "        self.vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape([3,1,1])\n",
    "        self.create(dropout)\n",
    "        \n",
    "    def create(self, dropout):\n",
    "        def vgg_preprocess(x, mean):\n",
    "            mean = np.array(mean)\n",
    "            x = x - mean\n",
    "            return x[:,:,::-1]\n",
    "        \n",
    "        model = self.model = Sequential()\n",
    "        \n",
    "        model.add(Lambda(vgg_preprocess, \n",
    "                         input_shape=(3, 244, 244), \n",
    "                         output_shape=(3, 244, 244),\n",
    "                         arguments = {'mean': self.vgg_mean.tolist()}\n",
    "                        ))\n",
    "        \n",
    "        model = add_conv_block(model, 2, 64)\n",
    "        model = add_conv_block(model, 2, 128)\n",
    "        model = add_conv_block(model, 3, 256)\n",
    "        model = add_conv_block(model, 3, 512)\n",
    "        model = add_conv_block(model, 3, 512)\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model = add_fc_block(model, dropout)\n",
    "        model = add_fc_block(model, dropout)\n",
    "        model.add(Dense(1000, activation='softmax'))\n",
    "        \n",
    "        model = model.load_weights(get_file(model_name, model_url+model_name, cache_subdir=cache_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load in data with generators\n",
    "Here I set up the generators for the training and validation work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1419 images belonging to 10 classes.\n",
      "Found 651 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "DEBUG = True\n",
    "data_dir = os.path.join(os.getcwd(), os.pardir, 'data')\n",
    "model_dir = os.path.join(os.getcwd(), os.pardir, 'models')\n",
    "if DEBUG == True:\n",
    "    path = os.path.join(data_dir, 'processed', 'sample')\n",
    "    batch_size = 4\n",
    "    epochs = 2\n",
    "elif DEBUG == False:\n",
    "    path = os.path.join(data_dir, 'processed')\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "\n",
    "train_path = os.path.join(path, 'train')\n",
    "val_path = os.path.join(path, 'valid')\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(244,244), \n",
    "                                                         batch_size=batch_size, \n",
    "                                                         shuffle=True)\n",
    "val_batches = ImageDataGenerator().flow_from_directory(val_path, \n",
    "                                                       target_size=(244,244), \n",
    "                                                       batch_size=batch_size, \n",
    "                                                       shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning the model\n",
    "* Now the top layer must be popped and replaced with a 10-output, which will correspond to our hot-encoding/softmax output\n",
    "* Then retrain model with new dense layer, which will be a good starting point for later fine tuning\n",
    "* Save the model, so that we can start toying with it in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1419/1419 [==============================] - 68s - loss: 1.6324 - acc: 0.6032 - val_loss: 4.8515 - val_acc: 0.1813\n",
      "Epoch 2/2\n",
      "1419/1419 [==============================] - 68s - loss: 0.5178 - acc: 0.8541 - val_loss: 4.1250 - val_acc: 0.2273\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "model = vgg16(dropout=0.5).model\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=Adam(lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(train_batches, \n",
    "                    samples_per_epoch=train_batches.nb_sample, \n",
    "                    nb_epoch=epochs, \n",
    "                    validation_data=val_batches, \n",
    "                    nb_val_samples=val_batches.nb_sample)\n",
    "\n",
    "model.save(os.path.join(model_dir, 'model_with_new_top.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New model architecture\n",
    "Now that we have the trained model, we should probably make all the FC layers trainable. Additionally, we can start playing with:\n",
    "* learning rate schedule\n",
    "* batchnorm\n",
    "* data augmentation\n",
    "* setting different epochs\n",
    "* some other kind of regularisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the model from when we saved it. Then:\n",
    "* Separate convolutional layers from fully connected ones\n",
    "* Make a new convolutional architecture with whatever we want to implement\n",
    "* Put them together\n",
    "* Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_model = load_model(os.path.join(os.getcwd(), \n",
    "                                    os.pardir, \n",
    "                                    'models', \n",
    "                                    'model_with_new_top.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Batch normalisation\n",
    "Let's implement batch normalisation first. It'll speed up our looking for the adequate learning rate. From [this link](https://github.com/fchollet/keras/issues/1802) we know that `BatchNorm()` needs to be applied after the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatten_index = [index for index,layer in enumerate(old_model.layers) if type(layer).__name__ == 'Flatten'][0]\n",
    "\n",
    "conv_model_layers = old_model.layers[1:flatten_index-1]\n",
    "conv_model = Sequential(conv_model_layers)\n",
    "\n",
    "def fc_model(dropout):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(MaxPooling2D(input_shape=conv_model.layers[-1].output_shape[1:]))\n",
    "#     model.layers[-1].name='maxpool_appended' # Shim because otherwise it would throw a dupe name error for no clear reason\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "Let's set up new batch generators, this time making use of augmented data. Remember, we only seek to augment our **training** input, no need to augment validation input (there's no learning taking place).\n",
    "`train_batches` generator is set to `False` because we're going to be saving it, and need reproducible inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Some minor debug settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "data_dir = os.path.join(os.getcwd(), os.pardir, 'data')\n",
    "model_dir = os.path.join(os.getcwd(), os.pardir, 'models')\n",
    "if DEBUG == True:\n",
    "    path = os.path.join(data_dir, 'processed', 'sample')\n",
    "    batch_size = 4\n",
    "    epochs = 2\n",
    "elif DEBUG == False:\n",
    "    path = os.path.join(data_dir, 'processed')\n",
    "    batch_size = 64\n",
    "    epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv stack output\n",
    "Using only the convolutional part of VGG16, I generate the predictions, based on some augmented data, and save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18873 images belonging to 10 classes.\n",
      "Found 18873 images belonging to 10 classes.\n",
      "Found 3551 images belonging to 10 classes.\n",
      "Predicting, this may take a while...\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(path, 'train')\n",
    "val_path = os.path.join(path, 'valid')\n",
    "\n",
    "train_image_gen = ImageDataGenerator(rotation_range=15,\n",
    "                                     height_shift_range=0.05,\n",
    "                                     width_shift_range=0.1,\n",
    "                                     shear_range = 0.1,\n",
    "                                     channel_shift_range=20,\n",
    "                                    )\n",
    "\n",
    "aug_train_batches = train_image_gen.flow_from_directory(train_path, \n",
    "                                                    target_size=(244,244), \n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)\n",
    "\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                    target_size=(244,244), \n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)\n",
    "\n",
    "val_batches = ImageDataGenerator().flow_from_directory(val_path, \n",
    "                                                       target_size=(244,244), \n",
    "                                                       batch_size=batch_size, \n",
    "                                                       shuffle=False)\n",
    "\n",
    "print 'Predicting, this may take a while...'\n",
    "conv_model_predictions_augmented = conv_model.predict_generator(aug_train_batches,\n",
    "                                                 aug_train_batches.nb_sample*2)\n",
    "conv_model_predictions = conv_model.predict_generator(train_batches,\n",
    "                                                 train_batches.nb_sample)\n",
    "val_predictions = conv_model.predict_generator(val_batches,\n",
    "                                               val_batches.nb_sample)\n",
    "\n",
    "print 'Done predicting!'\n",
    "# Concatenating augmented and non-augmented predictions\n",
    "conv_model_predictions = np.concatenate([conv_model_predictions_augmented, conv_model_predictions])\n",
    "\n",
    "prediction_labels = to_categorical(train_batches.classes)\n",
    "\n",
    "prediction_labels = np.concatenate([prediction_labels]*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save everything to disk\n",
    "Saving everything to disk so I don't need to generate it every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_array(location, array):\n",
    "    instance = bcolz.carray(array, rootdir=location, mode='w')\n",
    "    instance.flush()\n",
    "    \n",
    "def load_array(location):\n",
    "    return bcolz.open(location)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(os.path.join(model_dir, 'conv_predictions.bc'), conv_model_predictions)\n",
    "save_array(os.path.join(model_dir, 'conv_labels.bc'), prediction_labels)\n",
    "save_array(os.path.join(model_dir, 'val_predictions.bc'), val_predictions)\n",
    "save_array(os.path.join(model_dir, 'val_labels.bc'), to_categorical(val_batches.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train fully connected layers only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_predictions = load_array(os.path.join(model_dir, 'conv_predictions.bc'))\n",
    "conv_labels = load_array(os.path.join(model_dir, 'conv_labels.bc'))\n",
    "conv_val_predictions = load_array(os.path.join(model_dir, 'val_predictions.bc'))\n",
    "conv_val_labels = load_array(os.path.join(model_dir, 'val_labels.bc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4257 samples, validate on 651 samples\n",
      "Epoch 1/10\n",
      "4257/4257 [==============================] - 11s - loss: 2.2542 - acc: 0.2939 - val_loss: 1.8142 - val_acc: 0.3287\n",
      "Epoch 2/10\n",
      "4257/4257 [==============================] - 11s - loss: 1.4064 - acc: 0.5262 - val_loss: 1.7341 - val_acc: 0.3103\n",
      "Epoch 3/10\n",
      "4257/4257 [==============================] - 11s - loss: 1.0353 - acc: 0.6613 - val_loss: 1.3457 - val_acc: 0.5346\n",
      "Epoch 4/10\n",
      "4257/4257 [==============================] - 11s - loss: 0.8098 - acc: 0.7369 - val_loss: 1.3547 - val_acc: 0.5607\n",
      "Epoch 5/10\n",
      "4257/4257 [==============================] - 11s - loss: 0.6302 - acc: 0.7994 - val_loss: 1.1785 - val_acc: 0.6083\n",
      "Epoch 6/10\n",
      "4257/4257 [==============================] - 11s - loss: 0.5919 - acc: 0.8189 - val_loss: 1.0722 - val_acc: 0.7035\n",
      "Epoch 7/10\n",
      "4257/4257 [==============================] - 11s - loss: 0.5300 - acc: 0.8363 - val_loss: 1.1781 - val_acc: 0.5699\n",
      "Epoch 8/10\n",
      "4257/4257 [==============================] - 11s - loss: 0.4442 - acc: 0.8574 - val_loss: 1.3757 - val_acc: 0.5699\n",
      "Epoch 9/10\n",
      "4257/4257 [==============================] - 11s - loss: 0.3630 - acc: 0.8910 - val_loss: 1.1710 - val_acc: 0.5868\n",
      "Epoch 10/10\n",
      "4257/4257 [==============================] - 11s - loss: 0.3875 - acc: 0.8795 - val_loss: 1.2638 - val_acc: 0.5806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6235930810>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = 0.4\n",
    "model = fc_model(dropout)\n",
    "epochs = 10\n",
    "lr = 0.0001\n",
    "model.compile(optimizer=Adam(lr), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.optimizer.lr.set_value(lr)\n",
    "model.fit(conv_predictions,\n",
    "          conv_labels,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=epochs,\n",
    "          validation_data=(conv_val_predictions, conv_val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 15\n",
    "model.optimizer.lr.set_value(lr)\n",
    "model.fit(conv_predictions,\n",
    "          conv_labels,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=epochs,\n",
    "          validation_data=(conv_val_predictions, conv_val_labels))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
